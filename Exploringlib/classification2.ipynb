{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pri2k/BTP/blob/main/Exploringlib/classification2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS0cGlvm7GDm"
      },
      "source": [
        "Uploading files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "CDqJnSvv7DnA",
        "outputId": "864a35ee-c455-4690-f680-34cf5a0dd9ec"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b50180b5-a14d-4704-a231-f5320d00d962\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b50180b5-a14d-4704-a231-f5320d00d962\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving harvey_weinstein to harvey_weinstein (1)\n",
            "Saving demonetization_9 to demonetization_9 (1)\n",
            "Saving catalan_independence to catalan_independence (1)\n",
            "Saving ball_tampering to ball_tampering (1)\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Upload the file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the filename\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "# Read and join all paragraphs as one big text\n",
        "with open(filename, 'r', encoding='utf-8') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Step 2: Split lines and filter out '---------' and blank lines\n",
        "lines = content.strip().splitlines()\n",
        "filtered_lines = [line.strip() for line in lines if not line.strip().startswith('-') and line.strip() != '']\n",
        "\n",
        "# Step 3: Join into paragraphs and flatten\n",
        "combined_text = ' '.join(filtered_lines)\n",
        "\n",
        "\n",
        "# lines = content.strip().splitlines()\n",
        "# filtered_lines = [line.strip() for line in lines if not line.strip().startswith('-') and line.strip() != '']\n",
        "# combined_text = ' '.join(filtered_lines)\n",
        "\n",
        "\n",
        "# # Split on paragraph breaks (double newline), then join with space\n",
        "\n",
        "# paragraphs = content.strip().split('\\n\\n')\n",
        "# combined_text = ' '.join(paragraph.strip().replace('\\n', ' ') for paragraph in paragraphs)\n",
        "\n",
        "# print(f\" Combined {len(paragraphs)} paragraphs into one block of {len(combined_text.split())} words.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7HkuU_E_kQ4"
      },
      "source": [
        "Installing requirements (you only need to run it one time when running the code for the first time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ze1S_pQgdL6E",
        "outputId": "1669f30d-0841-4e02-adb7-951ceb476c68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rake-nltk in /usr/local/lib/python3.11/dist-packages (1.0.6)\n",
            "Requirement already satisfied: yake in /usr/local/lib/python3.11/dist-packages (0.4.8)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from rake-nltk) (3.9.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from yake) (0.9.0)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.11/dist-packages (from yake) (8.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from yake) (2.0.2)\n",
            "Requirement already satisfied: segtok in /usr/local/lib/python3.11/dist-packages (from yake) (1.5.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from yake) (3.5)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.11/dist-packages (from yake) (1.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.6.15)\n",
            "Requirement already satisfied: keybert in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from keybert) (2.0.2)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.11/dist-packages (from keybert) (13.9.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.11/dist-packages (from keybert) (1.6.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.52.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (2.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (3.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "!pip install rake-nltk yake\n",
        "# !pip uninstall -y transformers\n",
        "!pip install transformers --upgrade\n",
        "\n",
        "!pip install --upgrade huggingface_hub\n",
        "!pip install --upgrade keybert sentence-transformers\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWo6rHw7_nAv"
      },
      "source": [
        "import and extractors defining"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g15SurtVc2Qc",
        "outputId": "059e83c8-1f85-4a02-f60f-4365e8790aa0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "fatal: destination path 'all-MiniLM-L6-v2' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "TR-shNZR6rSW"
      },
      "outputs": [],
      "source": [
        "# RAKE\n",
        "from rake_nltk import Rake\n",
        "\n",
        "# YAKE\n",
        "import yake\n",
        "\n",
        "# KeyBERT\n",
        "from keybert import KeyBERT\n",
        "\n",
        "# Initialize models\n",
        "rake = Rake()\n",
        "kw_extractor_yake = yake.KeywordExtractor(lan=\"en\", top=10)\n",
        "kw_model = KeyBERT(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04cmu7wm_pip"
      },
      "source": [
        "functions for each extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Fv6tkky58Djb"
      },
      "outputs": [],
      "source": [
        "# def extract_keywords_rake(text, top_n=100):\n",
        "#     rake.extract_keywords_from_text(text)\n",
        "#     return rake.get_ranked_phrases_with_scores()[:top_n]\n",
        "\n",
        "def extract_keywords_rake(text, top_n=100, min_words=1, max_words=1):\n",
        "    rake.extract_keywords_from_text(text)\n",
        "    ranked_phrases = rake.get_ranked_phrases_with_scores()\n",
        "\n",
        "    seen = set()\n",
        "    filtered = []\n",
        "\n",
        "    for score, phrase in ranked_phrases:\n",
        "        phrase_clean = phrase.strip().lower()\n",
        "        word_count = len(phrase_clean.split())\n",
        "        if phrase_clean not in seen and min_words <= word_count <= max_words:\n",
        "            filtered.append((score, phrase))\n",
        "            seen.add(phrase_clean)\n",
        "        if len(filtered) >= top_n:\n",
        "            break\n",
        "\n",
        "    return filtered\n",
        "\n",
        "\n",
        "def extract_keywords_yake(text):\n",
        "    return kw_extractor_yake.extract_keywords(text)\n",
        "\n",
        "def extract_keywords_keybert(text):\n",
        "    return kw_model.extract_keywords(\n",
        "        text,\n",
        "        top_n=100,\n",
        "        stop_words='english',\n",
        "        keyphrase_ngram_range=(1, 2)\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An1FqSdw_sXy"
      },
      "source": [
        "Calling the functions and printing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEsbGWpu8PqD",
        "outputId": "142d9e6e-2cdf-4633-ea52-a9e4a7a36814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            " Extracting keywords from entire article collection\n",
            "\n",
            "---  RAKE Keywords ---\n",
            "empire (6.50)\n",
            "claims (6.20)\n",
            "saturday (5.67)\n",
            "stoller (5.00)\n",
            "1990 (5.00)\n",
            "october (4.89)\n",
            "expand (4.67)\n",
            "salma (4.50)\n",
            "chased (4.50)\n",
            "17 (4.50)\n",
            "16 (4.50)\n",
            "wednesday (4.33)\n",
            "since (4.33)\n",
            "details (4.33)\n",
            "weinsteins (4.25)\n",
            "action (4.20)\n",
            "bill (4.17)\n",
            "friday (4.14)\n",
            "director (4.12)\n",
            "thursday (4.07)\n",
            "tells (4.00)\n",
            "takes (4.00)\n",
            "oppress (4.00)\n",
            "named (4.00)\n",
            "editor (4.00)\n",
            "chief (4.00)\n",
            "actor (4.00)\n",
            "24 (4.00)\n",
            "23 (4.00)\n",
            "tuesday (3.85)\n",
            "monday (3.77)\n",
            "figures (3.75)\n",
            "variety (3.67)\n",
            "investigators (3.67)\n",
            "including (3.67)\n",
            "published (3.62)\n",
            "like (3.57)\n",
            "mogul (3.56)\n",
            "walk (3.50)\n",
            "started (3.50)\n",
            "spokeswoman (3.50)\n",
            "safe (3.50)\n",
            "president (3.50)\n",
            "month (3.50)\n",
            "mcgowan (3.50)\n",
            "investigation (3.50)\n",
            "dollars (3.50)\n",
            "december (3.50)\n",
            "confirmed (3.50)\n",
            "complicated (3.50)\n",
            "building (3.50)\n",
            "prosecutors (3.45)\n",
            "harvey (3.44)\n",
            "evans (3.43)\n",
            "times (3.40)\n",
            "used (3.40)\n",
            "magazine (3.38)\n",
            "two (3.36)\n",
            "‘ (3.36)\n",
            "star (3.33)\n",
            "mira (3.33)\n",
            "manhattan (3.33)\n",
            "makes (3.33)\n",
            "kantor (3.33)\n",
            "accounts (3.33)\n",
            "— (3.32)\n",
            ".” (3.26)\n",
            "ties (3.25)\n",
            "raped (3.25)\n",
            "change (3.25)\n",
            "least (3.20)\n",
            "cannes (3.20)\n",
            "brother (3.20)\n",
            "10 (3.20)\n",
            "” (3.18)\n",
            "investigating (3.17)\n",
            "ashley (3.14)\n",
            "advances (3.14)\n",
            "around (3.11)\n",
            "movie (3.09)\n",
            "winslet (3.00)\n",
            "wilson (3.00)\n",
            "uk (3.00)\n",
            "still (3.00)\n",
            "sorvino (3.00)\n",
            "sex (3.00)\n",
            "several (3.00)\n",
            "see (3.00)\n",
            "section (3.00)\n",
            "revealed (3.00)\n",
            "paid (3.00)\n",
            "oscar (3.00)\n",
            "officials (3.00)\n",
            "moment (3.00)\n",
            "include (3.00)\n",
            "house (3.00)\n",
            "goes (3.00)\n",
            "film (3.00)\n",
            "document (3.00)\n",
            "cruz (3.00)\n",
            "\n",
            "---  YAKE Keywords ---\n",
            "Harvey Weinstein (0.000054)\n",
            "Weinstein (0.000135)\n",
            "Weinstein Company (0.000201)\n",
            "producer Harvey Weinstein (0.000307)\n",
            "Harvey Weinsteins company (0.000494)\n",
            "mogul Harvey Weinstein (0.000556)\n",
            "York Times (0.000619)\n",
            "Harvey Weinstein Sexual (0.000693)\n",
            "Harvey (0.001229)\n",
            "years Harvey Weinstein (0.001381)\n",
            "\n",
            "---  KeyBERT Keywords ---\n",
            "weinstein allegations (0.6439)\n",
            "allegations weinstein (0.6176)\n",
            "weinstein accused (0.6015)\n",
            "weinsteins alleged (0.5991)\n",
            "rape weinstein (0.5976)\n",
            "weinstein raped (0.5970)\n",
            "weinstein assaulted (0.5939)\n",
            "accuses weinstein (0.5935)\n",
            "weinstein accusers (0.5933)\n",
            "weinstein alleged (0.5931)\n",
            "assaulted weinstein (0.5879)\n",
            "raped weinstein (0.5780)\n",
            "weinstein incidents (0.5772)\n",
            "weinstein harassment (0.5713)\n",
            "weinstein misconduct (0.5678)\n",
            "weinstein accuser (0.5670)\n",
            "weinstein raping (0.5588)\n",
            "weinstein harassed (0.5471)\n",
            "weinstein sexual (0.5469)\n",
            "investigating weinstein (0.5452)\n",
            "weinsteins sexual (0.5413)\n",
            "weinstein assault (0.5404)\n",
            "weinstein victims (0.5387)\n",
            "harassed weinstein (0.5364)\n",
            "weinstein investigation (0.5350)\n",
            "weinstein sexually (0.5230)\n",
            "york accuses (0.5228)\n",
            "weinstein denies (0.5115)\n",
            "weinstein inappropriate (0.5109)\n",
            "sex weinstein (0.5109)\n",
            "abuse weinstein (0.5064)\n",
            "assault allegations (0.5029)\n",
            "weinstein disputes (0.5028)\n",
            "yorker weinstein (0.5024)\n",
            "weinstein dating (0.5017)\n",
            "weinstein issues (0.5008)\n",
            "prosecutors manhattan (0.5004)\n",
            "details weinstein (0.4992)\n",
            "alleged rape (0.4989)\n",
            "rape allegations (0.4982)\n",
            "weinstein scandal (0.4954)\n",
            "weinstein actresses (0.4946)\n",
            "claims weinstein (0.4933)\n",
            "encounter weinstein (0.4907)\n",
            "lawsuit weinstein (0.4904)\n",
            "specific allegations (0.4900)\n",
            "investigated allegations (0.4868)\n",
            "harvey weinstein (0.4846)\n",
            "weinsteins lawyer (0.4841)\n",
            "harvey weinsteins (0.4829)\n",
            "allegations rape (0.4828)\n",
            "weinstein charged (0.4801)\n",
            "sexually assaulting (0.4782)\n",
            "weinstein denied (0.4751)\n",
            "accusations rape (0.4747)\n",
            "weinsteins behaviour (0.4742)\n",
            "manhattan investigating (0.4741)\n",
            "allegations violated (0.4733)\n",
            "include allegations (0.4732)\n",
            "weinstein story (0.4731)\n",
            "weinstein behaviour (0.4727)\n",
            "alleged sexual (0.4718)\n",
            "new allegations (0.4715)\n",
            "involved weinstein (0.4712)\n",
            "alleged behavior (0.4711)\n",
            "weinstein lawyer (0.4689)\n",
            "weinstein approached (0.4677)\n",
            "sexual assault (0.4661)\n",
            "accusations assault (0.4657)\n",
            "weinstein repeatedly (0.4651)\n",
            "weinstein forcibly (0.4635)\n",
            "case weinstein (0.4635)\n",
            "accusations sexually (0.4634)\n",
            "sues weinstein (0.4617)\n",
            "accused sexual (0.4607)\n",
            "weinstein fired (0.4605)\n",
            "accusations sexual (0.4604)\n",
            "weinstein suddenly (0.4595)\n",
            "investigating accusations (0.4585)\n",
            "fired weinstein (0.4579)\n",
            "harvey weinsteindawn (0.4578)\n",
            "accused sexually (0.4578)\n",
            "allegations harvey (0.4578)\n",
            "weinstein undressed (0.4572)\n",
            "weinstein complaints (0.4570)\n",
            "concerning weinstein (0.4566)\n",
            "sexual allegations (0.4566)\n",
            "weinstein denial (0.4564)\n",
            "behavior allegations (0.4544)\n",
            "weinstein reported (0.4541)\n",
            "harvey alleged (0.4541)\n",
            "weinstein doubts (0.4538)\n",
            "weinstein timeline (0.4524)\n",
            "weinstein attempt (0.4521)\n",
            "rapist accused (0.4517)\n",
            "decades weinsteins (0.4509)\n",
            "weinstein officers (0.4504)\n",
            "limitations prosecutors (0.4503)\n",
            "sexually assaulted (0.4488)\n",
            "alleged actions (0.4470)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n==============================\")\n",
        "print(\" Extracting keywords from entire article collection\\n\")\n",
        "\n",
        "print(\"---  RAKE Keywords ---\")\n",
        "for score, kw in extract_keywords_rake(combined_text):\n",
        "    print(f\"{kw} ({score:.2f})\")\n",
        "\n",
        "print(\"\\n---  YAKE Keywords ---\")\n",
        "for kw, score in extract_keywords_yake(combined_text):\n",
        "    print(f\"{kw} ({score:.6f})\")\n",
        "\n",
        "print(\"\\n---  KeyBERT Keywords ---\")\n",
        "for kw, score in extract_keywords_keybert(combined_text):\n",
        "    print(f\"{kw} ({score:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-0Pru54C3Wu",
        "outputId": "ba17840e-4e4a-491b-9ca7-fa9fe0bbebba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Succesfully stored\n"
          ]
        }
      ],
      "source": [
        "rake_keywords = extract_keywords_rake(combined_text)  # (score, keyword)\n",
        "yake_keywords = extract_keywords_yake(combined_text)  # (keyword, score)\n",
        "keybert_keywords = extract_keywords_keybert(combined_text)  # (keyword, score)\n",
        "print(\"Succesfully stored\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfokHvMniVY4"
      },
      "source": [
        "Downloading the extracted keywords\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "id": "NxwIuCbUWDpK",
        "outputId": "5843d59a-3354-4de4-ce3b-9a23a5f98aa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keywords saved to CSV files.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3e0874d9-e6fb-4772-aeb2-c7e7ebfe3edd\", \"Dem_rake_keywords.csv\", 1656)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d1f98965-8c2a-480a-b0c4-85695f6240ee\", \"Dem_yake_keywords.csv\", 420)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_04c91aa7-f37d-4b36-8866-1f24e9d6fbc2\", \"Dem_keybert_keywords.csv\", 2566)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert to DataFrames\n",
        "rake_df = pd.DataFrame(rake_keywords, columns=[\"Score\", \"Keyword\"])\n",
        "yake_df = pd.DataFrame(yake_keywords, columns=[\"Keyword\", \"Score\"])\n",
        "keybert_df = pd.DataFrame(keybert_keywords, columns=[\"Keyword\", \"Score\"])\n",
        "\n",
        "# Save to CSV files\n",
        "rake_df.to_csv(\"Dem_rake_keywords.csv\", index=False)\n",
        "yake_df.to_csv(\"Dem_yake_keywords.csv\", index=False)\n",
        "keybert_df.to_csv(\"Dem_keybert_keywords.csv\", index=False)\n",
        "\n",
        "print(\"Keywords saved to CSV files.\")\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.download(\"Dem_rake_keywords.csv\")\n",
        "files.download(\"Dem_yake_keywords.csv\")\n",
        "files.download(\"Dem_keybert_keywords.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjk8ispnDehR"
      },
      "source": [
        "CLASSIFY WHETHER TARGET OR NOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "J7gPNcmsDmb8"
      },
      "outputs": [],
      "source": [
        "def classify_keywords(keyword_list, method_keywords, threshold, method_name):\n",
        "    if method_name == \"RAKE\":\n",
        "        # RAKE gives (score, phrase), so we reverse it\n",
        "        kw_dict = {kw.lower(): score for score, kw in method_keywords}\n",
        "    else:\n",
        "        kw_dict = {kw.lower(): score for kw, score in method_keywords}\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for keyword in keyword_list:\n",
        "        keyword_lower = keyword.lower()\n",
        "        found = False\n",
        "\n",
        "        for phrase, score in kw_dict.items():\n",
        "            if keyword_lower in phrase:\n",
        "                # print(f\"[DEBUG] {keyword_lower} matched phrase '{phrase}' with score {score}\")\n",
        "                if method_name in [\"RAKE\", \"KeyBERT\"]:\n",
        "                    if score >= threshold:\n",
        "                        found = True\n",
        "                        break\n",
        "                else:  # YAKE: lower is better\n",
        "                    if round(score, 6) < round(threshold, 6):\n",
        "                        found = True\n",
        "                        break\n",
        "\n",
        "        results[keyword] = found\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KP6UzlsKWBJ"
      },
      "source": [
        "For every new article make sure to change the keyword to test and the threshold values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ri6Y2AWDD0Y9",
        "outputId": "6caea88b-7bd5-4494-8dc1-ce94146d9bbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rake classification\n",
            "susana vera “ falling apart ” puigdemont appeared → False\n",
            "mobile world congress → False\n",
            "former parliament speaker carme forcadell → False\n",
            "\n",
            "\n",
            "Yake classification\n",
            "Spain → False\n",
            "Catalan independence → False\n",
            "\n",
            "\n",
            "Keybert classification\n",
            "catalonia want independence → False\n",
            "regional election catalonia → False\n",
            "catalonians voted → False\n",
            "spain called elections → False\n"
          ]
        }
      ],
      "source": [
        "keywords_to_test = [\"Catalan independence\"]\n",
        "raketest=[\"susana vera “ falling apart ” puigdemont appeared\",\"mobile world congress\",\"former parliament speaker carme forcadell\"]\n",
        "yaketest=[\"Spain\",\"Catalan independence\"]\n",
        "keyberttest=[\"catalonia want independence\",\"regional election catalonia\",\"catalonians voted\",\"spain called elections\"]\n",
        "\n",
        "rake_threshold = 30\n",
        "yake_threshold = 0.000277  # e.g. raw score, lower means more important\n",
        "keybert_threshold = 0.63\n",
        "\n",
        "rakewords= classify_keywords(raketest, rake_keywords, rake_threshold, \"RAKE\")\n",
        "yakewords= classify_keywords(yaketest, yake_keywords, yake_threshold, \"YAKE\")\n",
        "keybertwords= classify_keywords(keyberttest, keybert_keywords, keybert_threshold, \"KeyBERT\")\n",
        "\n",
        "print(\"Rake classification\")\n",
        "for word, is_target in rakewords.items():\n",
        "    print(f\"{word} → {is_target}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Yake classification\")\n",
        "for word, is_target in yakewords.items():\n",
        "    print(f\"{word} → {is_target}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Keybert classification\")\n",
        "for word, is_target in keybertwords.items():\n",
        "    print(f\"{word} → {is_target}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}